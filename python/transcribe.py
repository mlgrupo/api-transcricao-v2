#!/usr/bin/env python3
"""
TranscriÃ§Ã£o Simples e Eficiente
- Sem aceleraÃ§Ã£o de Ã¡udio (causa travamentos)
- Chunks de 10 minutos para evitar travamentos
- Paralelismo real com mÃºltiplos processos
- Logs detalhados de progresso
- OtimizaÃ§Ã£o mÃ¡xima de CPU - TODOS OS CORES
"""

import sys
import json
import logging
import whisper
from text_processor import TextProcessor, TextProcessingRules
from pydub import AudioSegment
import os
import multiprocessing
import torch
import numpy as np
# Adicionando pyannote para diarizaÃ§Ã£o
from pyannote.audio import Pipeline as PyannotePipeline
from huggingface_hub import hf_hub_download
import concurrent.futures
import subprocess
import threading

# FunÃ§Ã£o utilitÃ¡ria para carregar pipeline de diarizaÃ§Ã£o

def load_pyannote_pipeline():
    """Pipeline bÃ¡sico do pyannote.audio sem configuraÃ§Ãµes customizadas."""
    hf_token = os.environ.get("HUGGINGFACE_TOKEN")
    if not hf_token:
        raise RuntimeError("VariÃ¡vel de ambiente HUGGINGFACE_TOKEN nÃ£o definida.")
    
    # Usar pipeline padrÃ£o sem configuraÃ§Ãµes customizadas
    pipeline = PyannotePipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1", 
        use_auth_token=hf_token
    )
    
    device = torch.device("cpu")
    pipeline.to(device)
    return pipeline

# Configurar para usar TODOS os CPUs disponÃ­veis
def setup_cpu_optimization():
    """Configura otimizaÃ§Ã£o mÃ¡xima de CPU para 8 vCPUs do CCX33."""
    cpu_count = min(8, multiprocessing.cpu_count())  # CCX33 tem 8 vCPUs
    
    # ConfiguraÃ§Ãµes agressivas para mÃ¡xima velocidade
    os.environ["OMP_NUM_THREADS"] = str(cpu_count)
    os.environ["MKL_NUM_THREADS"] = str(cpu_count)
    os.environ["OPENBLAS_NUM_THREADS"] = str(cpu_count)
    os.environ["NUMEXPR_NUM_THREADS"] = str(cpu_count)
    os.environ["VECLIB_MAXIMUM_THREADS"] = str(cpu_count)
    os.environ["BLIS_NUM_THREADS"] = str(cpu_count)
    os.environ["MKL_DYNAMIC"] = "FALSE"
    os.environ["OMP_DYNAMIC"] = "FALSE"
    os.environ["GOMP_CPU_AFFINITY"] = "0-7"  # Usar todos os 8 cores
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
    
    # ConfiguraÃ§Ãµes PyTorch para CPU
    torch.set_num_threads(cpu_count)
    torch.set_num_interop_threads(cpu_count)
    
    return cpu_count

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def basic_text_processor():
    rules = TextProcessingRules(
        capitalize_sentences=True,
        fix_spaces=True,
        ensure_final_punctuation=True,
        normalize_numbers=False,
        fix_common_errors=False,
        normalize_punctuation=True,
        capitalize_words=['Brasil', 'SÃ£o Paulo', 'Rio de Janeiro'],
        common_replacements={}
    )
    return TextProcessor(rules)

def split_audio_streaming(file_path, chunk_duration_ms=30 * 60 * 1000):  # 30 minutos para CCX33
    """Corta o Ã¡udio em blocos de 30 minutos (otimizado para CCX33 com 8 vCPUs)."""
    audio = AudioSegment.from_file(file_path)
    for i in range(0, len(audio), chunk_duration_ms):
        chunk = audio[i:i + chunk_duration_ms]
        chunk_path = f"{file_path}_chunk_{i // chunk_duration_ms}.mp3"
        chunk.export(chunk_path, format="mp3", bitrate="64k")  # Bitrate baixo = mais rÃ¡pido
        yield chunk_path, i // chunk_duration_ms

def extract_audio_if_needed(input_path):
    """Extrai Ã¡udio em 8kHz mono para mÃ¡xima velocidade."""
    audio_exts = ['.wav', '.mp3', '.flac', '.ogg', '.m4a']
    video_exts = ['.mp4', '.mkv', '.mov', '.avi', '.webm']
    ext = os.path.splitext(input_path)[1].lower()
    if ext in audio_exts:
        return input_path
    if ext in video_exts:
        output_path = input_path + '_audio.wav'
        cmd = [
            'ffmpeg', '-y', '-i', input_path,
            '-vn', '-acodec', 'pcm_s16le', '-ar', '8000', '-ac', '1',  # 8kHz mono
            '-af', 'highpass=f=200,lowpass=f=3000',  # Filtro para reduzir ruÃ­do
            output_path
        ]
        subprocess.run(cmd, check=True)
        return output_path
    return input_path

# FunÃ§Ã£o para transcrever um chunk (para uso em paralelo)
def transcribe_chunk(args):
    chunk_path, chunk_index, model, text_processor = args
    result = model.transcribe(
        chunk_path,
        language="pt",
        word_timestamps=True,
        initial_prompt=(
            "Transcreva em portuguÃªs do Brasil. "
            "Use linguagem formal e evite redundÃ¢ncias. "
            "Corrija erros comuns e normalize nÃºmeros."
        ),
        fp16=False,
        verbose=False,
        condition_on_previous_text=False,
        compression_ratio_threshold=2.4,
        logprob_threshold=-1.0,
        no_speech_threshold=0.6
    )
    chunk_start_time = chunk_index * 30 * 60
    segments = []
    for segment in result.get("segments", []):
        segment["start"] += chunk_start_time
        segment["end"] += chunk_start_time
        processed_text = text_processor.process(segment["text"])
        segment["text"] = processed_text
        segments.append(segment)
    try:
        os.remove(chunk_path)
    except Exception:
        pass
    return segments

def transcribe_audio(audio_path):
    try:
        # NOVO: extrair Ã¡udio se necessÃ¡rio
        original_path = audio_path
        audio_path = extract_audio_if_needed(audio_path)
        temp_audio = (audio_path != original_path) and audio_path.endswith('_audio.wav')
        
        # Configurar otimizaÃ§Ã£o mÃ¡xima de CPU
        cpu_count = setup_cpu_optimization()
        logger.info(f"ðŸš€ OtimizaÃ§Ã£o de CPU configurada: {cpu_count} cores disponÃ­veis")
        
        text_processor = basic_text_processor()
        logger.info("âœ… Text processor inicializado")
        
        logger.info("ðŸ”„ Carregando modelo Whisper Small...")
        model = whisper.load_model("small")
        logger.info("âœ… Modelo Whisper Small carregado com sucesso")

        # --- DiarizaÃ§Ã£o do Ã¡udio completo ---
        skip_diarization = os.environ.get("SKIP_DIARIZATION", "false").lower() == "true"
        
        if skip_diarization:
            logger.info("â­ï¸ Pulando diarizaÃ§Ã£o (SKIP_DIARIZATION=true). Usando segmentaÃ§Ã£o simples...")
            diarized_segments = create_simple_segments(audio_path)
        else:
            logger.info("ðŸ”Š Executando diarizaÃ§Ã£o de locutores (pyannote, CPU)...")
            diarization_pipeline = load_pyannote_pipeline()
            diarized_segments = diarize_audio(audio_path, diarization_pipeline)

        logger.info(f"âœ… DiarizaÃ§Ã£o concluÃ­da: {len(diarized_segments)} segmentos encontrados")
        chunk_args = []
        logger.info("ðŸ“‚ Dividindo Ã¡udio em chunks de 15 minutos...")
        for chunk_path, chunk_index in split_audio_streaming(audio_path):
            chunk_args.append((chunk_path, chunk_index, model, text_processor))
        whisper_segments = []
        # Processar chunks em paralelo usando todos os nÃºcleos disponÃ­veis
        logger.info(f"âš¡ Transcrevendo {len(chunk_args)} chunks em paralelo com {cpu_count} workers...")
        with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count) as executor:
            for chunk_result in executor.map(transcribe_chunk, chunk_args):
                whisper_segments.extend(chunk_result)
        logger.info(f"âœ… TranscriÃ§Ã£o paralela concluÃ­da: {len(whisper_segments)} segmentos")
        # --- Alinhar segmentos do Whisper com locutores ---
        logger.info("ðŸ”— Alinhando segmentos da transcriÃ§Ã£o com locutores...")
        aligned = align_segments_with_speakers(whisper_segments, diarized_segments)
        # Mapear SPEAKER_XX para LOCUTOR_X
        speaker_map = {}
        speaker_count = 1
        formatted_segments = []
        def format_time(seconds):
            h = int(seconds // 3600)
            m = int((seconds % 3600) // 60)
            s = int(seconds % 60)
            return f"{h:02}:{m:02}:{s:02}"
        for seg in aligned:
            spk = seg['speaker']
            if spk not in speaker_map:
                speaker_map[spk] = f"LOCUTOR_{speaker_count}"
                speaker_count += 1
            locutor = speaker_map[spk]
            start_str = format_time(seg['start'])
            formatted_segments.append(f"{start_str} {locutor}: {seg['text']}")
        formatted_text = "\n".join(formatted_segments)
        logger.info(f"ðŸŽ‰ TranscriÃ§Ã£o e diarizaÃ§Ã£o concluÃ­das!")
        logger.info(f"ðŸ“Š Resumo: {len(chunk_args)} chunks, {len(aligned)} segmentos, {len(formatted_text)} caracteres")
        result = json.dumps({
            "status": "success",
            "text": formatted_text.strip(),
            "segments": aligned,
            "chunks": len(chunk_args),
            "language": "pt"
        }, ensure_ascii=False)
        # Limpeza automÃ¡tica do Ã¡udio temporÃ¡rio extraÃ­do
        if temp_audio and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
                logger.info(f"ðŸ—‘ï¸ Ãudio temporÃ¡rio removido: {audio_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ Erro ao remover Ã¡udio temporÃ¡rio: {e}")
        return result
    except Exception as e:
        # Limpeza tambÃ©m em caso de erro
        if 'audio_path' in locals() and 'temp_audio' in locals() and temp_audio and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
                logger.info(f"ðŸ—‘ï¸ Ãudio temporÃ¡rio removido apÃ³s erro: {audio_path}")
            except Exception:
                pass
        return json.dumps({
            "status": "error",
            "error": str(e)
        }, ensure_ascii=False)

def diarize_audio(audio_path, pipeline=None, max_duration_minutes=30):
    """DiarizaÃ§Ã£o ultra-otimizada para velocidade."""
    if pipeline is None:
        pipeline = load_pyannote_pipeline()
    
    audio = AudioSegment.from_file(audio_path)
    duration_minutes = len(audio) / (1000 * 60)
    
    logger.info(f"ðŸš€ DiarizaÃ§Ã£o ULTRA-RÃPIDA iniciada para {duration_minutes:.1f}min de Ã¡udio")
    
    diarization_done = threading.Event()
    def progress_log():
        if not diarization_done.is_set():
            logger.info("âš¡ DiarizaÃ§Ã£o ULTRA-RÃPIDA em andamento...")
            threading.Timer(30, progress_log).start()  # Log a cada 30 segundos
    
    progress_log()
    
    try:
        # Timeout mais agressivo
        timeout_seconds = min(duration_minutes * 60, 1800)  # MÃ¡ximo 30 min
        logger.info(f"â±ï¸ Timeout otimizado: {timeout_seconds/60:.1f} minutos")
        
        diarization = pipeline(audio_path)
        diarized_segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            diarized_segments.append({
                'speaker': speaker,
                'start': float(turn.start),
                'end': float(turn.end)
            })
        
        diarization_done.set()
        logger.info(f"âš¡ DiarizaÃ§Ã£o ULTRA-RÃPIDA concluÃ­da: {len(diarized_segments)} segmentos")
        return diarized_segments
        
    except Exception as e:
        diarization_done.set()
        logger.error(f"âŒ Erro na diarizaÃ§Ã£o: {e}")
        logger.info("ðŸ”„ Usando fallback otimizado...")
        return create_simple_segments(audio_path, segment_duration=60)  # Segmentos de 1 minuto

def create_simple_segments(audio_path, segment_duration=30):
    """Cria segmentos simples baseados em tempo quando diarizaÃ§Ã£o falha."""
    audio = AudioSegment.from_file(audio_path)
    duration_seconds = len(audio) / 1000
    segments = []
    
    for i in range(0, int(duration_seconds), segment_duration):
        segments.append({
            'speaker': f'SPEAKER_{i//segment_duration:02d}',
            'start': float(i),
            'end': float(min(i + segment_duration, duration_seconds))
        })
    
    return segments

def align_segments_with_speakers(whisper_segments, diarized_segments):
    """Alinha os segmentos do Whisper com os segmentos diarizados por maior interseÃ§Ã£o temporal."""
    def find_best_speaker(start, end):
        best_speaker = None
        max_overlap = 0
        for seg in diarized_segments:
            overlap = max(0, min(end, seg['end']) - max(start, seg['start']))
            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = seg['speaker']
        return best_speaker or 'SPEAKER_00'

    aligned = []
    for seg in whisper_segments:
        speaker = find_best_speaker(seg['start'], seg['end'])
        aligned.append({
            'speaker': speaker,
            'start': seg['start'],
            'end': seg['end'],
            'text': seg['text']
        })
    return aligned

if __name__ == "__main__":
    if len(sys.argv) != 2:
        sys.stdout.buffer.write(json.dumps({
            "status": "error",
            "error": "Please provide the audio file path"
        }, ensure_ascii=False).encode('utf-8'))
        sys.exit(1)

    audio_path = sys.argv[1]
    output = transcribe_audio(audio_path)
    sys.stdout.buffer.write(output.encode('utf-8')) 