#!/usr/bin/env python3
"""
TranscriÃ§Ã£o Simples e Eficiente
- Sem aceleraÃ§Ã£o de Ã¡udio (causa travamentos)
- Chunks de 10 minutos para evitar travamentos
- Paralelismo real com mÃºltiplos processos
- Logs detalhados de progresso
- OtimizaÃ§Ã£o mÃ¡xima de CPU - TODOS OS CORES
"""

import sys
import json
import logging
import whisper
from text_processor import TextProcessor, TextProcessingRules
from pydub import AudioSegment
import os
import multiprocessing
import torch
import numpy as np
# Adicionando pyannote para diarizaÃ§Ã£o
from pyannote.audio import Pipeline as PyannotePipeline
from huggingface_hub import hf_hub_download
import concurrent.futures
import subprocess
import threading

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def basic_text_processor():
    rules = TextProcessingRules(
        capitalize_sentences=True,
        fix_spaces=True,
        ensure_final_punctuation=True,
        normalize_numbers=False,
        fix_common_errors=False,
        normalize_punctuation=True,
        capitalize_words=['Brasil', 'SÃ£o Paulo', 'Rio de Janeiro'],
        common_replacements={}
    )
    return TextProcessor(rules)

def split_audio_streaming(file_path, chunk_duration_ms=15 * 60 * 1000):
    """Corta o Ã¡udio em blocos de X segundos (default: 15min para maior eficiÃªncia em CPU)."""
    audio = AudioSegment.from_file(file_path)
    for i in range(0, len(audio), chunk_duration_ms):
        chunk = audio[i:i + chunk_duration_ms]
        chunk_path = f"{file_path}_chunk_{i // chunk_duration_ms}.mp3"
        chunk.export(chunk_path, format="mp3")
        yield chunk_path, i // chunk_duration_ms

def extract_audio_if_needed(input_path):
    """Se for vÃ­deo, extrai o Ã¡udio para WAV mono 16kHz e retorna o novo caminho. Se jÃ¡ for Ã¡udio, retorna o original."""
    audio_exts = ['.wav', '.mp3', '.flac', '.ogg', '.m4a']
    video_exts = ['.mp4', '.mkv', '.mov', '.avi', '.webm']
    ext = os.path.splitext(input_path)[1].lower()
    if ext in audio_exts:
        return input_path
    if ext in video_exts:
        output_path = input_path + '_audio.wav'
        cmd = [
            'ffmpeg', '-y', '-i', input_path,
            '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', output_path
        ]
        subprocess.run(cmd, check=True)
        return output_path
    # Se extensÃ£o desconhecida, tenta processar como Ã¡udio
    return input_path

# FunÃ§Ã£o para transcrever um chunk (para uso em paralelo)
def transcribe_chunk(args):
    chunk_path, chunk_index, model, text_processor = args
    result = model.transcribe(
        chunk_path,
        language="pt",
        word_timestamps=True,
        initial_prompt=(
            "Transcreva em portuguÃªs do Brasil. "
            "Use linguagem formal e evite redundÃ¢ncias. "
            "Corrija erros comuns e normalize nÃºmeros."
        ),
        fp16=False,
        verbose=False,
        condition_on_previous_text=False,
        compression_ratio_threshold=2.4,
        logprob_threshold=-1.0,
        no_speech_threshold=0.6
    )
    chunk_start_time = chunk_index * 15 * 60
    segments = []
    for segment in result.get("segments", []):
        segment["start"] += chunk_start_time
        segment["end"] += chunk_start_time
        processed_text = text_processor.process(segment["text"])
        segment["text"] = processed_text
        segments.append(segment)
    try:
        os.remove(chunk_path)
    except Exception:
        pass
    return segments

def transcribe_audio(audio_path):
    try:
        # NOVO: extrair Ã¡udio se necessÃ¡rio
        original_path = audio_path
        audio_path = extract_audio_if_needed(audio_path)
        temp_audio = (audio_path != original_path) and audio_path.endswith('_audio.wav')
        
        # Configurar otimizaÃ§Ã£o mÃ¡xima de CPU
        cpu_count = setup_cpu_optimization()
        logger.info(f"ðŸš€ OtimizaÃ§Ã£o de CPU configurada: {cpu_count} cores disponÃ­veis")
        
        text_processor = basic_text_processor()
        logger.info("âœ… Text processor inicializado")
        
        logger.info("ðŸ”„ Carregando modelo Whisper Small...")
        model = whisper.load_model("small")
        logger.info("âœ… Modelo Whisper Small carregado com sucesso")

        # --- DiarizaÃ§Ã£o do Ã¡udio completo ---
        skip_diarization = os.environ.get("SKIP_DIARIZATION", "false").lower() == "true"
        
        if skip_diarization:
            logger.info("â­ï¸ Pulando diarizaÃ§Ã£o (SKIP_DIARIZATION=true). Usando segmentaÃ§Ã£o simples...")
            diarized_segments = create_simple_segments(audio_path)
        else:
            logger.info("ðŸ”Š Executando diarizaÃ§Ã£o de locutores (pyannote, CPU)...")
            diarization_pipeline = load_pyannote_pipeline()
            diarized_segments = diarize_audio(audio_path, diarization_pipeline)

        logger.info(f"âœ… DiarizaÃ§Ã£o concluÃ­da: {len(diarized_segments)} segmentos encontrados")
        
        chunk_args = []
        logger.info("ðŸ“‚ Dividindo Ã¡udio em chunks de 15 minutos...")
        for chunk_path, chunk_index in split_audio_streaming(audio_path):
            chunk_args.append((chunk_path, chunk_index, model, text_processor))
        
        whisper_segments = []
        # Processar chunks em paralelo usando apenas 1 worker no servidor
        logger.info(f"âš¡ Transcrevendo {len(chunk_args)} chunks com 1 worker (sequencial)...")
        with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:
            for chunk_result in executor.map(transcribe_chunk, chunk_args):
                whisper_segments.extend(chunk_result)
        logger.info(f"âœ… TranscriÃ§Ã£o concluÃ­da: {len(whisper_segments)} segmentos")

        # --- Alinhar segmentos do Whisper com locutores ---
        logger.info("ðŸ”— Alinhando segmentos da transcriÃ§Ã£o com locutores...")
        aligned = align_segments_with_speakers(whisper_segments, diarized_segments)
        # Mapear SPEAKER_XX para LOCUTOR_X
        speaker_map = {}
        speaker_count = 1
        formatted_segments = []
        def format_time(seconds):
            h = int(seconds // 3600)
            m = int((seconds % 3600) // 60)
            s = int(seconds % 60)
            return f"{h:02}:{m:02}:{s:02}"
        for seg in aligned:
            spk = seg['speaker']
            if spk not in speaker_map:
                speaker_map[spk] = f"LOCUTOR_{speaker_count}"
                speaker_count += 1
            locutor = speaker_map[spk]
            start_str = format_time(seg['start'])
            formatted_segments.append(f"{start_str} {locutor}: {seg['text']}")
        formatted_text = "\n".join(formatted_segments)
        logger.info(f"ðŸŽ‰ TranscriÃ§Ã£o e diarizaÃ§Ã£o concluÃ­das!")
        logger.info(f"ðŸ“Š Resumo: {len(chunk_args)} chunks, {len(aligned)} segmentos, {len(formatted_text)} caracteres")
        result = json.dumps({
            "status": "success",
            "text": formatted_text.strip(),
            "segments": aligned,
            "chunks": len(chunk_args),
            "language": "pt"
        }, ensure_ascii=False)
        # Limpeza automÃ¡tica do Ã¡udio temporÃ¡rio extraÃ­do
        if temp_audio and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
                logger.info(f"ðŸ—‘ï¸ Ãudio temporÃ¡rio removido: {audio_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ Erro ao remover Ã¡udio temporÃ¡rio: {e}")
        return result
    except Exception as e:
        logger.error(f"âŒ Erro na transcriÃ§Ã£o: {e}")
        # Limpeza em caso de erro
        if temp_audio and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
                logger.info(f"ðŸ—‘ï¸ Ãudio temporÃ¡rio removido apÃ³s erro: {audio_path}")
            except Exception:
                pass
        raise

def load_pyannote_pipeline():
    """Carrega o pipeline de diarizaÃ§Ã£o do pyannote usando o token HuggingFace."""
    hf_token = os.environ.get("HUGGINGFACE_TOKEN")
    if not hf_token:
        raise RuntimeError("VariÃ¡vel de ambiente HUGGINGFACE_TOKEN nÃ£o definida. Cadastre seu token do HuggingFace.")
    pipeline = PyannotePipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=hf_token)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pipeline.to(device)
    return pipeline

def setup_cpu_optimization():
    """Configura otimizaÃ§Ã£o mÃ¡xima de CPU"""
    # Detectar nÃºmero de CPUs
    cpu_count = multiprocessing.cpu_count()
    
    # Configurar PyTorch para usar todos os cores
    try:
        if torch.cuda.is_available():
            torch.set_num_threads(cpu_count)
            logger.info(f"PyTorch CUDA configurado para {cpu_count} threads")
        else:
            # Para CPU, usar todos os cores disponÃ­veis
            torch.set_num_threads(cpu_count)
            torch.set_num_interop_threads(cpu_count)
            logger.info(f"PyTorch CPU configurado para {cpu_count} threads")
    except Exception as e:
        logger.warning(f"Erro ao configurar PyTorch threads: {e}")
        pass
    
    # Configurar NumPy para usar todos os cores (compatibilidade)
    try:
        np.set_num_threads(cpu_count)
        logger.info(f"NumPy configurado para {cpu_count} threads")
    except AttributeError:
        # VersÃµes mais antigas do NumPy nÃ£o tÃªm set_num_threads
        logger.info(f"NumPy nÃ£o suporta set_num_threads, usando configuraÃ§Ã£o padrÃ£o")
        pass
    
    # Configurar variÃ¡veis de ambiente para bibliotecas BLAS
    os.environ['OMP_NUM_THREADS'] = str(cpu_count)
    os.environ['MKL_NUM_THREADS'] = str(cpu_count)
    os.environ['OPENBLAS_NUM_THREADS'] = str(cpu_count)
    os.environ['VECLIB_MAXIMUM_THREADS'] = str(cpu_count)
    os.environ['NUMEXPR_NUM_THREADS'] = str(cpu_count)
    os.environ['BLIS_NUM_THREADS'] = str(cpu_count)
    
    # Desabilitar thread dinÃ¢mico para melhor performance
    os.environ['MKL_DYNAMIC'] = 'FALSE'
    os.environ['OMP_DYNAMIC'] = 'FALSE'
    
    return cpu_count

def diarize_audio(audio_path, pipeline=None):
    """Executa diarizaÃ§Ã£o e retorna lista de segmentos: [{'speaker': 'SPEAKER_00', 'start': float, 'end': float}]"""
    if pipeline is None:
        pipeline = load_pyannote_pipeline()
    diarization = pipeline(audio_path)
    diarized_segments = []
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        diarized_segments.append({
            'speaker': speaker,
            'start': float(turn.start),
            'end': float(turn.end)
        })
    return diarized_segments

def create_simple_segments(audio_path, segment_duration=30):
    """Cria segmentos simples baseados em tempo quando diarizaÃ§Ã£o falha."""
    audio = AudioSegment.from_file(audio_path)
    duration_seconds = len(audio) / 1000
    segments = []
    
    for i in range(0, int(duration_seconds), segment_duration):
        segments.append({
            'speaker': f'SPEAKER_{i//segment_duration:02d}',
            'start': float(i),
            'end': float(min(i + segment_duration, duration_seconds))
        })
    
    return segments

def align_segments_with_speakers(whisper_segments, diarized_segments):
    """Alinha os segmentos do Whisper com os segmentos diarizados por maior interseÃ§Ã£o temporal."""
    def find_best_speaker(start, end):
        best_speaker = None
        max_overlap = 0
        for seg in diarized_segments:
            overlap = max(0, min(end, seg['end']) - max(start, seg['start']))
            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = seg['speaker']
        return best_speaker or 'SPEAKER_00'

    aligned = []
    for seg in whisper_segments:
        speaker = find_best_speaker(seg['start'], seg['end'])
        aligned.append({
            'speaker': speaker,
            'start': seg['start'],
            'end': seg['end'],
            'text': seg['text']
        })
    return aligned

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Uso: python transcribe.py <caminho_do_audio>")
        sys.exit(1)
    
    audio_path = sys.argv[1]
    try:
        result = transcribe_audio(audio_path)
        print(result)
    except Exception as e:
        error_result = json.dumps({
            "status": "error",
            "error": str(e)
        }, ensure_ascii=False)
        print(error_result)
        sys.exit(1) 