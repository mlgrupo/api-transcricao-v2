#!/usr/bin/env python3
"""
Diariza√ß√£o EQUILIBRADA - Sempre 3 chunks para m√°xima estabilidade
FOCO: Qualidade m√°xima da diariza√ß√£o, divis√£o fixa em 3 chunks
"""
from pyannote.audio import Pipeline
import os
import sys
from typing import List
import logging
import signal
import time
from pydub import AudioSegment
import torch
import tempfile
import psutil
import gc
import math

# Token HuggingFace
HF_TOKEN = os.environ.get("HF_TOKEN")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DiarizationSegment:
    def __init__(self, start: float, end: float, speaker: str):
        self.start = start
        self.end = end
        self.speaker = speaker

    def to_dict(self):
        return {"start": self.start, "end": self.end, "speaker": self.speaker}

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Timeout na diariza√ß√£o")

def intelligent_fallback_diarization(audio_path: str, duration: float) -> List[DiarizationSegment]:
    """Fallback inteligente que mant√©m fidelidade usando an√°lise de √°udio"""
    logger.info("üîÑ Executando diariza√ß√£o inteligente de fallback...")
    
    try:
        audio = AudioSegment.from_file(audio_path)
        
        # An√°lise mais sofisticada para detectar mudan√ßas de speaker
        # 1. Detectar sil√™ncios para segmenta√ß√£o inicial
        silence_thresh = audio.dBFS - 16  # Mais sens√≠vel a mudan√ßas
        min_silence_len = 800  # 0.8 segundos
        
        # 2. An√°lise de energia para detectar transi√ß√µes
        chunks = []
        chunk_size = 10000  # 10 segundos para an√°lise
        overlap = 2000     # 2 segundos de overlap
        
        for i in range(0, len(audio), chunk_size - overlap):
            chunk = audio[i:i + chunk_size]
            if len(chunk) >= 3000:  # M√≠nimo 3 segundos
                chunks.append((i / 1000.0, chunk))
        
        segments = []
        current_speaker = 0
        speaker_duration = 0
        
        for start_time, chunk in chunks:
            chunk_duration = len(chunk) / 1000.0
            
            # Detectar mudan√ßa de speaker baseado em:
            # 1. Dura√ß√£o do speaker atual (15-45 segundos t√≠pico)
            # 2. Mudan√ßas significativas na energia do √°udio
            # 3. Pausas/sil√™ncios
            
            should_change_speaker = False
            
            # Mudan√ßa por tempo (evitar speakers muito longos)
            if speaker_duration > 45.0:
                should_change_speaker = True
            # Mudan√ßa por energia (diferen√ßa significativa)
            elif speaker_duration > 8.0:  # M√≠nimo 8s por speaker
                try:
                    chunk_energy = chunk.dBFS
                    # Se energia mudou muito, pode ser speaker diferente
                    if hasattr(segments, '__len__') and len(segments) > 0:
                        prev_chunk_start = max(0, int((start_time - 5) * 1000))
                        prev_chunk_end = int(start_time * 1000)
                        if prev_chunk_end > prev_chunk_start:
                            prev_chunk = audio[prev_chunk_start:prev_chunk_end]
                            if abs(chunk_energy - prev_chunk.dBFS) > 8:  # 8dB diferen√ßa
                                should_change_speaker = True
                except:
                    pass
            
            if should_change_speaker:
                current_speaker = (current_speaker + 1) % 8  # At√© 8 speakers
                speaker_duration = 0
            
            # Criar segmento
            end_time = start_time + chunk_duration
            segments.append(DiarizationSegment(
                start_time, 
                min(end_time, duration),
                f"SPEAKER_{current_speaker:02d}"
            ))
            
            speaker_duration += chunk_duration
        
        # Mesclar segmentos consecutivos do mesmo speaker
        if segments:
            merged = [segments[0]]
            for seg in segments[1:]:
                last = merged[-1]
                if last.speaker == seg.speaker and seg.start - last.end <= 3.0:
                    last.end = seg.end
                else:
                    merged.append(seg)
            segments = merged
        
        # Se ainda temos poucos segmentos, adicionar mais varia√ß√£o
        if len(set(seg.speaker for seg in segments)) < 2 and duration > 300:
            logger.info("Aumentando varia√ß√£o de speakers para √°udio longo...")
            enhanced_segments = []
            for seg in segments:
                seg_duration = seg.end - seg.start
                if seg_duration > 60:  # Segmentos > 1min, dividir
                    mid_point = seg.start + seg_duration / 2
                    new_speaker = f"SPEAKER_{(int(seg.speaker.split('_')[1]) + 1) % 6:02d}"
                    enhanced_segments.append(DiarizationSegment(seg.start, mid_point, seg.speaker))
                    enhanced_segments.append(DiarizationSegment(mid_point, seg.end, new_speaker))
                else:
                    enhanced_segments.append(seg)
            segments = enhanced_segments
        
        speakers = set(seg.speaker for seg in segments)
        logger.info(f"‚úÖ Fallback inteligente: {len(segments)} segmentos, {len(speakers)} speakers")
        return segments
        
    except Exception as e:
        logger.error(f"Erro no fallback inteligente: {e}")
        # √öltimo recurso: segmenta√ß√£o temporal simples
        segments = []
        segment_duration = min(30.0, duration / 4)  # M√°ximo 4 segmentos
        current_time = 0.0
        speaker_id = 0
        
        while current_time < duration:
            end_time = min(current_time + segment_duration, duration)
            segments.append(DiarizationSegment(
                current_time, end_time, f"SPEAKER_{speaker_id:02d}"
            ))
            current_time = end_time
            speaker_id = (speaker_id + 1) % 3
        
        return segments

class ServerResourceManager:
    """Gerencia recursos priorizando QUALIDADE da diariza√ß√£o"""
    
    def __init__(self):
        self.max_cpu_cores = min(6, os.cpu_count())
        self.max_ram_gb = 28
        
    def check_resources(self) -> dict:
        """Verifica recursos dispon√≠veis do servidor"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_available_gb': memory.available / (1024**3),
            'safe_to_process': cpu_percent < 85 and memory.percent < 80
        }
    
    def get_chunk_timeout(self, chunk_duration: float) -> int:
        """Calcula timeout din√¢mico baseado na dura√ß√£o do chunk"""
        # Timeout base: 2 minutos por minuto de √°udio + buffer
        base_timeout = int(chunk_duration / 60 * 2)  # 2 min por min de √°udio
        buffer_timeout = 5  # 5 minutos de buffer
        
        # M√≠nimo 10 minutos, m√°ximo 30 minutos
        timeout = max(10, min(30, base_timeout + buffer_timeout))
        logger.info(f"Timeout calculado para chunk de {chunk_duration/60:.1f}min: {timeout}min")
        return timeout
    
    def configure_torch(self):
        """Configura√ß√£o otimizada para QUALIDADE"""
        torch.set_num_threads(self.max_cpu_cores)
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()

def get_audio_duration(audio_path: str) -> float:
    """Retorna dura√ß√£o do √°udio em segundos"""
    try:
        audio = AudioSegment.from_file(audio_path)
        return len(audio) / 1000.0
    except Exception as e:
        logger.warning(f"Erro ao obter dura√ß√£o do √°udio: {e}")
        return 0

def split_audio_into_3_chunks(audio_path: str) -> List[tuple]:
    """NOVO: Sempre divide em exatamente 3 chunks para m√°xima estabilidade"""
    try:
        audio = AudioSegment.from_file(audio_path)
        total_duration = len(audio) / 1000.0
        
        logger.info(f"üî™ DIVIS√ÉO FIXA: Dividindo √°udio de {total_duration/60:.1f}min em exatamente 3 chunks")
        
        # Calcular dura√ß√£o de cada chunk
        chunk_duration = total_duration / 3.0
        overlap = min(30, chunk_duration * 0.1)  # 10% overlap, m√°ximo 30s
        
        logger.info(f"üìä Cada chunk ter√° ~{chunk_duration/60:.1f}min com overlap de {overlap}s")
        
        chunks = []
        
        # Chunk 1: in√≠cio at√© 1/3 + overlap
        start_1 = 0
        end_1 = chunk_duration + overlap
        chunk_1_audio = audio[int(start_1 * 1000):int(min(end_1, total_duration) * 1000)]
        
        # Chunk 2: 1/3 - overlap at√© 2/3 + overlap  
        start_2 = chunk_duration - overlap
        end_2 = (chunk_duration * 2) + overlap
        chunk_2_audio = audio[int(start_2 * 1000):int(min(end_2, total_duration) * 1000)]
        
        # Chunk 3: 2/3 - overlap at√© o final
        start_3 = (chunk_duration * 2) - overlap
        end_3 = total_duration
        chunk_3_audio = audio[int(start_3 * 1000):int(end_3 * 1000)]
        
        # Salvar chunks tempor√°rios
        for i, (chunk_audio, start_time, end_time) in enumerate([
            (chunk_1_audio, start_1, min(end_1, total_duration)),
            (chunk_2_audio, start_2, min(end_2, total_duration)), 
            (chunk_3_audio, start_3, end_3)
        ], 1):
            with tempfile.NamedTemporaryFile(suffix=f'_chunk{i}.wav', delete=False) as temp_file:
                chunk_audio.export(temp_file.name, format='wav', parameters=["-ac", "1", "-ar", "16000"])
                chunk_info = (temp_file.name, start_time, end_time)
                chunks.append(chunk_info)
                logger.info(f"‚úÖ Chunk {i}: {start_time/60:.1f}min - {end_time/60:.1f}min ({(end_time-start_time)/60:.1f}min)")
        
        logger.info(f"üéØ Divis√£o conclu√≠da: 3 chunks criados com sucesso")
        return chunks
        
    except Exception as e:
        logger.error(f"Erro ao dividir √°udio em 3 chunks: {e}")
        return [(audio_path, 0.0, get_audio_duration(audio_path))]

def diarize_chunk_optimized(pipeline, chunk_path: str, chunk_info: str, timeout_minutes: int = 15) -> List[DiarizationSegment]:
    """QUALIDADE M√ÅXIMA: Configura√ß√µes otimizadas para fidelidade"""
    segments = []
    
    try:
        # Timeout espec√≠fico para este chunk
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_minutes * 60)
        
        logger.info(f"üîÑ Processando {chunk_info} (timeout: {timeout_minutes}min)")
        
        # CONFIGURA√á√ÉO PARA M√ÅXIMA QUALIDADE
        diarization = pipeline(
            chunk_path,
            min_speakers=1,
            max_speakers=8,
        )
        
        signal.alarm(0)
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append(DiarizationSegment(turn.start, turn.end, speaker))
        
        logger.info(f"‚úÖ {chunk_info} processado: {len(segments)} segmentos")
        
        # Limpeza de mem√≥ria ap√≥s chunk
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return segments
        
    except TimeoutException:
        signal.alarm(0)
        logger.error(f"‚ùå Timeout no {chunk_info} ({timeout_minutes}min) - usando fallback inteligente")
        
        # FALLBACK INTELIGENTE em caso de timeout
        chunk_duration = get_audio_duration(chunk_path)
        return intelligent_fallback_diarization(chunk_path, chunk_duration)
        
    except Exception as e:
        signal.alarm(0)
        logger.error(f"‚ùå Erro no {chunk_info}: {e} - usando fallback inteligente")
        
        # FALLBACK INTELIGENTE em caso de erro
        chunk_duration = get_audio_duration(chunk_path)
        return intelligent_fallback_diarization(chunk_path, chunk_duration)

def merge_3_chunks_advanced(chunk_results: List[tuple]) -> List[DiarizationSegment]:
    """MELHORADO: Mesclagem espec√≠fica para 3 chunks"""
    if len(chunk_results) != 3:
        logger.warning(f"‚ö†Ô∏è Esperado 3 chunks, recebido {len(chunk_results)}")
    
    all_segments = []
    speaker_mapping = {}
    next_speaker_id = 0
    
    logger.info("üîó Mesclando 3 chunks com preserva√ß√£o de identidade...")
    
    # Processar cada chunk
    for chunk_idx, (segments, chunk_start, chunk_end) in enumerate(chunk_results):
        chunk_speakers = {}
        chunk_name = f"Chunk {chunk_idx + 1}"
        
        logger.info(f"üìù Processando {chunk_name}: {len(segments)} segmentos")
        
        for segment in segments:
            global_start = segment.start + chunk_start
            global_end = segment.end + chunk_start
            
            if segment.speaker not in chunk_speakers:
                if chunk_idx == 0:
                    # Primeiro chunk - estabelecer baseline
                    global_speaker = f"SPEAKER_{next_speaker_id:02d}"
                    chunk_speakers[segment.speaker] = global_speaker
                    next_speaker_id += 1
                else:
                    # Chunks 2 e 3 - an√°lise de continuidade
                    best_match = None
                    best_score = 0
                    
                    # Procurar por continuidade temporal
                    for existing_speaker in speaker_mapping.values():
                        # Encontrar √∫ltimo segmento deste speaker
                        last_end = 0
                        for prev_seg in all_segments:
                            if prev_seg.speaker == existing_speaker:
                                last_end = max(last_end, prev_seg.end)
                        
                        time_gap = global_start - last_end
                        
                        # Score baseado em proximidade
                        if time_gap < 180:  # 3 minutos de toler√¢ncia
                            score = max(0, 1 - (time_gap / 180))
                            if score > best_score:
                                best_score = score
                                best_match = existing_speaker
                    
                    if best_match and best_score > 0.3:
                        chunk_speakers[segment.speaker] = best_match
                        logger.info(f"üîó Speaker {segment.speaker} mapeado para {best_match} (score: {best_score:.2f})")
                    else:
                        # Novo speaker
                        global_speaker = f"SPEAKER_{next_speaker_id:02d}"
                        chunk_speakers[segment.speaker] = global_speaker
                        speaker_mapping[f"{chunk_idx}_{segment.speaker}"] = global_speaker
                        next_speaker_id += 1
                        logger.info(f"üÜï Novo speaker: {segment.speaker} ‚Üí {global_speaker}")
            
            all_segments.append(DiarizationSegment(
                global_start, global_end, chunk_speakers[segment.speaker]
            ))
    
    # Ordenar por tempo
    all_segments.sort(key=lambda x: x.start)
    
    # Mesclagem final preservando micro-pausas
    if not all_segments:
        return []
    
    merged = [all_segments[0]]
    for segment in all_segments[1:]:
        last = merged[-1]
        if (last.speaker == segment.speaker and 
            segment.start - last.end <= 3.0):  # 3s para pausas naturais
            last.end = max(last.end, segment.end)
        else:
            merged.append(segment)
    
    speakers = set(seg.speaker for seg in merged)
    logger.info(f"‚úÖ Mesclagem de 3 chunks conclu√≠da: {len(merged)} segmentos, {len(speakers)} speakers")
    
    return merged

def diarize_audio(audio_path: str) -> List[DiarizationSegment]:
    """
    üéØ DIARIZA√á√ÉO FIXA EM 3 CHUNKS - M√°xima estabilidade e qualidade
    """
    hf_token = HF_TOKEN
    if not hf_token:
        raise ValueError("Token HuggingFace obrigat√≥rio!")
    
    # Inicializar gerenciador de recursos
    resource_manager = ServerResourceManager()
    
    # Verificar recursos iniciais
    resources = resource_manager.check_resources()
    logger.info(f"üñ•Ô∏è SERVIDOR: CPU {resources['cpu_percent']:.1f}%, RAM {resources['memory_percent']:.1f}% ({resources['memory_available_gb']:.1f}GB livre)")
    
    duration = get_audio_duration(audio_path)
    logger.info(f"üé§ DIARIZA√á√ÉO 3-CHUNKS: √Åudio de {duration/60:.1f} minutos")
    
    # Fallback apenas para √°udios EXTREMAMENTE longos (>3 horas)
    if duration > 10800:  # 3 horas
        logger.warning("‚ö†Ô∏è √Åudio extremamente longo - usando diariza√ß√£o inteligente")
        return intelligent_fallback_diarization(audio_path, duration)
    
    try:
        # Configurar PyTorch para qualidade m√°xima
        resource_manager.configure_torch()
        
        # Carregar pipeline
        logger.info("üîß Carregando pipeline otimizado para qualidade...")
        start_time = time.time()
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(600)  # 10 minutos para carregar
        
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )
            
            if torch.cuda.is_available():
                logger.info("üöÄ GPU detectada - usando acelera√ß√£o")
                pipeline = pipeline.to(torch.device("cuda"))
            else:
                logger.info(f"üñ•Ô∏è CPU otimizada ({resource_manager.max_cpu_cores} cores)")
                pipeline = pipeline.to(torch.device("cpu"))
            
            signal.alarm(0)
            
        except TimeoutException:
            logger.error("‚ùå Timeout no carregamento do pipeline")
            return intelligent_fallback_diarization(audio_path, duration)
        
        load_time = time.time() - start_time
        logger.info(f"‚úÖ Pipeline carregado em {load_time:.1f}s")
        
        # Processamento direto para √°udios curtos (‚â§20 min)
        if duration <= 1200:  # 20 minutos
            logger.info("‚ö° Processamento direto para √°udio curto")
            
            try:
                timeout_min = resource_manager.get_chunk_timeout(duration)
                segments = diarize_chunk_optimized(pipeline, audio_path, "√°udio completo", timeout_min)
                speakers = set(seg.speaker for seg in segments)
                logger.info(f"‚úÖ PROCESSAMENTO DIRETO: {len(segments)} segmentos, {len(speakers)} speakers")
                return segments
                
            except Exception as e:
                logger.error(f"Falha no processamento direto: {e}")
                return intelligent_fallback_diarization(audio_path, duration)
        
        else:  # Processamento em 3 chunks
            logger.info("üî™ Divis√£o em 3 chunks para √°udio longo")
            
            chunks = split_audio_into_3_chunks(audio_path)
            chunk_results = []
            temp_files = []
            
            try:
                for i, (chunk_path, start_time_chunk, end_time_chunk) in enumerate(chunks):
                    temp_files.append(chunk_path)
                    
                    # Verificar recursos antes de cada chunk
                    resources = resource_manager.check_resources()
                    if resources['memory_percent'] > 85:
                        logger.warning("üßπ RAM alta - limpando mem√≥ria")
                        gc.collect()
                        time.sleep(2)
                    
                    chunk_duration = end_time_chunk - start_time_chunk
                    timeout_min = resource_manager.get_chunk_timeout(chunk_duration)
                    chunk_info = f"Chunk {i+1}/3"
                    
                    logger.info(f"üöÄ Iniciando {chunk_info} (RAM: {resources['memory_percent']:.1f}%)")
                    
                    # Processar chunk
                    segments = diarize_chunk_optimized(pipeline, chunk_path, chunk_info, timeout_min)
                    chunk_results.append((segments, start_time_chunk, end_time_chunk))
                
                # Mesclar 3 chunks
                final_segments = merge_3_chunks_advanced(chunk_results)
                
                speakers = set(seg.speaker for seg in final_segments)
                logger.info(f"üéØ PROCESSAMENTO 3-CHUNKS CONCLU√çDO: {len(final_segments)} segmentos, {len(speakers)} speakers")
                return final_segments
                
            finally:
                # Limpeza de arquivos tempor√°rios
                for temp_file in temp_files:
                    try:
                        if os.path.exists(temp_file):
                            os.unlink(temp_file)
                    except:
                        pass
                
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico: {e}")
        return intelligent_fallback_diarization(audio_path, duration)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso: python diarization.py <audio_path>")
        sys.exit(1)
    
    audio_path = sys.argv[1]
    
    try:
        logger.info(f"üéØ Iniciando diariza√ß√£o FIXA-3-CHUNKS: {audio_path}")
        segments = diarize_audio(audio_path)
        
        # Output
        for seg in segments:
            print(seg.to_dict())
            
    except Exception as e:
        logger.error(f"‚ùå Erro: {e}")
        sys.exit(1)