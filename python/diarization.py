#!/usr/bin/env python3
"""
DiarizaÃ§Ã£o EQUILIBRADA - Sempre 3 chunks para mÃ¡xima estabilidade
FOCO: Qualidade mÃ¡xima da diarizaÃ§Ã£o, divisÃ£o fixa em 3 chunks
"""
from pyannote.audio import Pipeline
import os
import sys
from typing import List
import logging
import signal
import time
from pydub import AudioSegment
import torch
import tempfile
import psutil
import gc
import math

# Token HuggingFace
HF_TOKEN = os.environ.get("HF_TOKEN")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DiarizationSegment:
    def __init__(self, start: float, end: float, speaker: str):
        self.start = start
        self.end = end
        self.speaker = speaker

    def to_dict(self):
        return {"start": self.start, "end": self.end, "speaker": self.speaker}

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Timeout na diarizaÃ§Ã£o")

def intelligent_fallback_diarization(audio_path: str, duration: float) -> List[DiarizationSegment]:
    """Fallback inteligente que mantÃ©m fidelidade usando anÃ¡lise de Ã¡udio"""
    logger.info("ğŸ”„ Executando diarizaÃ§Ã£o inteligente de fallback...")
    
    try:
        audio = AudioSegment.from_file(audio_path)
        
        # AnÃ¡lise mais sofisticada para detectar mudanÃ§as de speaker
        # 1. Detectar silÃªncios para segmentaÃ§Ã£o inicial
        silence_thresh = audio.dBFS - 16  # Mais sensÃ­vel a mudanÃ§as
        min_silence_len = 800  # 0.8 segundos
        
        # 2. AnÃ¡lise de energia para detectar transiÃ§Ãµes
        chunks = []
        chunk_size = 10000  # 10 segundos para anÃ¡lise
        overlap = 2000     # 2 segundos de overlap
        
        for i in range(0, len(audio), chunk_size - overlap):
            chunk = audio[i:i + chunk_size]
            if len(chunk) >= 3000:  # MÃ­nimo 3 segundos
                chunks.append((i / 1000.0, chunk))
        
        segments = []
        current_speaker = 0
        speaker_duration = 0
        
        for start_time, chunk in chunks:
            chunk_duration = len(chunk) / 1000.0
            
            # Detectar mudanÃ§a de speaker baseado em:
            # 1. DuraÃ§Ã£o do speaker atual (15-45 segundos tÃ­pico)
            # 2. MudanÃ§as significativas na energia do Ã¡udio
            # 3. Pausas/silÃªncios
            
            should_change_speaker = False
            
            # MudanÃ§a por tempo (evitar speakers muito longos)
            if speaker_duration > 45.0:
                should_change_speaker = True
            # MudanÃ§a por energia (diferenÃ§a significativa)
            elif speaker_duration > 8.0:  # MÃ­nimo 8s por speaker
                try:
                    chunk_energy = chunk.dBFS
                    # Se energia mudou muito, pode ser speaker diferente
                    if hasattr(segments, '__len__') and len(segments) > 0:
                        prev_chunk_start = max(0, int((start_time - 5) * 1000))
                        prev_chunk_end = int(start_time * 1000)
                        if prev_chunk_end > prev_chunk_start:
                            prev_chunk = audio[prev_chunk_start:prev_chunk_end]
                            if abs(chunk_energy - prev_chunk.dBFS) > 8:  # 8dB diferenÃ§a
                                should_change_speaker = True
                except:
                    pass
            
            if should_change_speaker:
                current_speaker = (current_speaker + 1) % 8  # AtÃ© 8 speakers
                speaker_duration = 0
            
            # Criar segmento
            end_time = start_time + chunk_duration
            segments.append(DiarizationSegment(
                start_time, 
                min(end_time, duration),
                f"SPEAKER_{current_speaker:02d}"
            ))
            
            speaker_duration += chunk_duration
        
        # Mesclar segmentos consecutivos do mesmo speaker
        if segments:
            merged = [segments[0]]
            for seg in segments[1:]:
                last = merged[-1]
                if last.speaker == seg.speaker and seg.start - last.end <= 3.0:
                    last.end = seg.end
                else:
                    merged.append(seg)
            segments = merged
        
        # Se ainda temos poucos segmentos, adicionar mais variaÃ§Ã£o
        if len(set(seg.speaker for seg in segments)) < 2 and duration > 300:
            logger.info("Aumentando variaÃ§Ã£o de speakers para Ã¡udio longo...")
            enhanced_segments = []
            for seg in segments:
                seg_duration = seg.end - seg.start
                if seg_duration > 60:  # Segmentos > 1min, dividir
                    mid_point = seg.start + seg_duration / 2
                    new_speaker = f"SPEAKER_{(int(seg.speaker.split('_')[1]) + 1) % 6:02d}"
                    enhanced_segments.append(DiarizationSegment(seg.start, mid_point, seg.speaker))
                    enhanced_segments.append(DiarizationSegment(mid_point, seg.end, new_speaker))
                else:
                    enhanced_segments.append(seg)
            segments = enhanced_segments
        
        speakers = set(seg.speaker for seg in segments)
        logger.info(f"âœ… Fallback inteligente: {len(segments)} segmentos, {len(speakers)} speakers")
        return segments
        
    except Exception as e:
        logger.error(f"Erro no fallback inteligente: {e}")
        # Ãšltimo recurso: segmentaÃ§Ã£o temporal simples
        segments = []
        segment_duration = min(30.0, duration / 4)  # MÃ¡ximo 4 segmentos
        current_time = 0.0
        speaker_id = 0
        
        while current_time < duration:
            end_time = min(current_time + segment_duration, duration)
            segments.append(DiarizationSegment(
                current_time, end_time, f"SPEAKER_{speaker_id:02d}"
            ))
            current_time = end_time
            speaker_id = (speaker_id + 1) % 3
        
        return segments

class ServerResourceManager:
    """Gerencia recursos priorizando QUALIDADE da diarizaÃ§Ã£o"""
    
    def __init__(self):
        self.max_cpu_cores = min(6, os.cpu_count())
        self.max_ram_gb = 28
        
    def check_resources(self) -> dict:
        """Verifica recursos disponÃ­veis do servidor"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_available_gb': memory.available / (1024**3),
            'safe_to_process': cpu_percent < 85 and memory.percent < 80
        }
    
    def get_chunk_timeout(self, chunk_duration: float) -> int:
        """Calcula timeout dinÃ¢mico baseado na duraÃ§Ã£o do chunk"""
        # Timeout base: 2 minutos por minuto de Ã¡udio + buffer
        base_timeout = int(chunk_duration / 60 * 2)  # 2 min por min de Ã¡udio
        buffer_timeout = 5  # 5 minutos de buffer
        
        # MÃ­nimo 10 minutos, mÃ¡ximo 30 minutos
        timeout = max(10, min(30, base_timeout + buffer_timeout))
        logger.info(f"Timeout calculado para chunk de {chunk_duration/60:.1f}min: {timeout}min")
        return timeout
    
    def configure_torch(self):
        """ConfiguraÃ§Ã£o otimizada para QUALIDADE"""
        torch.set_num_threads(self.max_cpu_cores)
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()

def get_audio_duration(audio_path: str) -> float:
    """Retorna duraÃ§Ã£o do Ã¡udio em segundos"""
    try:
        audio = AudioSegment.from_file(audio_path)
        return len(audio) / 1000.0
    except Exception as e:
        logger.warning(f"Erro ao obter duraÃ§Ã£o do Ã¡udio: {e}")
        return 0

def split_audio_into_3_chunks(audio_path: str) -> List[tuple]:
    """NOVO: Sempre divide em exatamente 3 chunks para mÃ¡xima estabilidade"""
    try:
        audio = AudioSegment.from_file(audio_path)
        total_duration = len(audio) / 1000.0
        
        logger.info(f"ğŸ”ª DIVISÃƒO FIXA: Dividindo Ã¡udio de {total_duration/60:.1f}min em exatamente 3 chunks")
        
        # Calcular duraÃ§Ã£o de cada chunk
        chunk_duration = total_duration / 3.0
        overlap = min(30, chunk_duration * 0.1)  # 10% overlap, mÃ¡ximo 30s
        
        logger.info(f"ğŸ“Š Cada chunk terÃ¡ ~{chunk_duration/60:.1f}min com overlap de {overlap}s")
        
        chunks = []
        
        # Chunk 1: inÃ­cio atÃ© 1/3 + overlap
        start_1 = 0
        end_1 = chunk_duration + overlap
        chunk_1_audio = audio[int(start_1 * 1000):int(min(end_1, total_duration) * 1000)]
        
        # Chunk 2: 1/3 - overlap atÃ© 2/3 + overlap  
        start_2 = chunk_duration - overlap
        end_2 = (chunk_duration * 2) + overlap
        chunk_2_audio = audio[int(start_2 * 1000):int(min(end_2, total_duration) * 1000)]
        
        # Chunk 3: 2/3 - overlap atÃ© o final
        start_3 = (chunk_duration * 2) - overlap
        end_3 = total_duration
        chunk_3_audio = audio[int(start_3 * 1000):int(end_3 * 1000)]
        
        # Salvar chunks temporÃ¡rios
        for i, (chunk_audio, start_time, end_time) in enumerate([
            (chunk_1_audio, start_1, min(end_1, total_duration)),
            (chunk_2_audio, start_2, min(end_2, total_duration)), 
            (chunk_3_audio, start_3, end_3)
        ], 1):
            with tempfile.NamedTemporaryFile(suffix=f'_chunk{i}.wav', delete=False) as temp_file:
                chunk_audio.export(temp_file.name, format='wav', parameters=["-ac", "1", "-ar", "16000"])
                chunk_info = (temp_file.name, start_time, end_time)
                chunks.append(chunk_info)
                logger.info(f"âœ… Chunk {i}: {start_time/60:.1f}min - {end_time/60:.1f}min ({(end_time-start_time)/60:.1f}min)")
        
        logger.info(f"ğŸ¯ DivisÃ£o concluÃ­da: 3 chunks criados com sucesso")
        return chunks
        
    except Exception as e:
        logger.error(f"Erro ao dividir Ã¡udio em 3 chunks: {e}")
        return [(audio_path, 0.0, get_audio_duration(audio_path))]

def diarize_chunk_optimized(pipeline, chunk_path: str, chunk_info: str, timeout_minutes: int = 15) -> List[DiarizationSegment]:
    """QUALIDADE MÃXIMA: ConfiguraÃ§Ãµes otimizadas para fidelidade"""
    segments = []
    
    try:
        # Timeout especÃ­fico para este chunk
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_minutes * 60)
        
        logger.info(f"ğŸ”„ Processando {chunk_info} (timeout: {timeout_minutes}min)")
        
        # CONFIGURAÃ‡ÃƒO PARA MÃXIMA QUALIDADE
        diarization = pipeline(
            chunk_path,
            min_speakers=1,
            max_speakers=8,
        )
        
        signal.alarm(0)
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append(DiarizationSegment(turn.start, turn.end, speaker))
        
        logger.info(f"âœ… {chunk_info} processado: {len(segments)} segmentos")
        
        # Limpeza de memÃ³ria apÃ³s chunk
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return segments
        
    except TimeoutException:
        signal.alarm(0)
        logger.error(f"âŒ Timeout no {chunk_info} ({timeout_minutes}min) - usando fallback inteligente")
        
        # FALLBACK INTELIGENTE em caso de timeout
        chunk_duration = get_audio_duration(chunk_path)
        return intelligent_fallback_diarization(chunk_path, chunk_duration)
        
    except Exception as e:
        signal.alarm(0)
        logger.error(f"âŒ Erro no {chunk_info}: {e} - usando fallback inteligente")
        
        # FALLBACK INTELIGENTE em caso de erro
        chunk_duration = get_audio_duration(chunk_path)
        return intelligent_fallback_diarization(chunk_path, chunk_duration)

def merge_3_chunks_advanced(chunk_results: List[tuple]) -> List[DiarizationSegment]:
    """MELHORADO: Mesclagem especÃ­fica para 3 chunks"""
    if len(chunk_results) != 3:
        logger.warning(f"âš ï¸ Esperado 3 chunks, recebido {len(chunk_results)}")
    
    all_segments = []
    speaker_mapping = {}
    next_speaker_id = 0
    
    logger.info("ğŸ”— Mesclando 3 chunks com preservaÃ§Ã£o de identidade...")
    
    # Processar cada chunk
    for chunk_idx, (segments, chunk_start, chunk_end) in enumerate(chunk_results):
        chunk_speakers = {}
        chunk_name = f"Chunk {chunk_idx + 1}"
        
        logger.info(f"ğŸ“ Processando {chunk_name}: {len(segments)} segmentos")
        
        for segment in segments:
            global_start = segment.start + chunk_start
            global_end = segment.end + chunk_start
            
            if segment.speaker not in chunk_speakers:
                if chunk_idx == 0:
                    # Primeiro chunk - estabelecer baseline
                    global_speaker = f"SPEAKER_{next_speaker_id:02d}"
                    chunk_speakers[segment.speaker] = global_speaker
                    next_speaker_id += 1
                else:
                    # Chunks 2 e 3 - anÃ¡lise de continuidade
                    best_match = None
                    best_score = 0
                    
                    # Procurar por continuidade temporal
                    for existing_speaker in speaker_mapping.values():
                        # Encontrar Ãºltimo segmento deste speaker
                        last_end = 0
                        for prev_seg in all_segments:
                            if prev_seg.speaker == existing_speaker:
                                last_end = max(last_end, prev_seg.end)
                        
                        time_gap = global_start - last_end
                        
                        # Score baseado em proximidade
                        if time_gap < 180:  # 3 minutos de tolerÃ¢ncia
                            score = max(0, 1 - (time_gap / 180))
                            if score > best_score:
                                best_score = score
                                best_match = existing_speaker
                    
                    if best_match and best_score > 0.3:
                        chunk_speakers[segment.speaker] = best_match
                        logger.info(f"ğŸ”— Speaker {segment.speaker} mapeado para {best_match} (score: {best_score:.2f})")
                    else:
                        # Novo speaker
                        global_speaker = f"SPEAKER_{next_speaker_id:02d}"
                        chunk_speakers[segment.speaker] = global_speaker
                        speaker_mapping[f"{chunk_idx}_{segment.speaker}"] = global_speaker
                        next_speaker_id += 1
                        logger.info(f"ğŸ†• Novo speaker: {segment.speaker} â†’ {global_speaker}")
            
            all_segments.append(DiarizationSegment(
                global_start, global_end, chunk_speakers[segment.speaker]
            ))
    
    # Ordenar por tempo
    all_segments.sort(key=lambda x: x.start)
    
    # Mesclagem final preservando micro-pausas
    if not all_segments:
        return []
    
    merged = [all_segments[0]]
    for segment in all_segments[1:]:
        last = merged[-1]
        if (last.speaker == segment.speaker and 
            segment.start - last.end <= 3.0):  # 3s para pausas naturais
            last.end = max(last.end, segment.end)
        else:
            merged.append(segment)
    
    speakers = set(seg.speaker for seg in merged)
    logger.info(f"âœ… Mesclagem de 3 chunks concluÃ­da: {len(merged)} segmentos, {len(speakers)} speakers")
    
    return merged

def diarize_audio(audio_path: str) -> List[DiarizationSegment]:
    """
    ğŸ¯ DIARIZAÃ‡ÃƒO FIXA EM 3 CHUNKS - MÃ¡xima estabilidade e qualidade
    """
    hf_token = HF_TOKEN
    if not hf_token:
        raise ValueError("Token HuggingFace obrigatÃ³rio!")
    
    # Inicializar gerenciador de recursos
    resource_manager = ServerResourceManager()
    
    # Verificar recursos iniciais
    resources = resource_manager.check_resources()
    logger.info(f"ğŸ–¥ï¸ SERVIDOR: CPU {resources['cpu_percent']:.1f}%, RAM {resources['memory_percent']:.1f}% ({resources['memory_available_gb']:.1f}GB livre)")
    
    duration = get_audio_duration(audio_path)
    logger.info(f"ğŸ¤ DIARIZAÃ‡ÃƒO 3-CHUNKS: Ãudio de {duration/60:.1f} minutos")
    
    # Fallback apenas para Ã¡udios EXTREMAMENTE longos (>3 horas)
    if duration > 10800:  # 3 horas
        logger.warning("âš ï¸ Ãudio extremamente longo - usando diarizaÃ§Ã£o inteligente")
        return intelligent_fallback_diarization(audio_path, duration)
    
    try:
        # Configurar PyTorch para qualidade mÃ¡xima
        resource_manager.configure_torch()
        
        # Carregar pipeline
        logger.info("ğŸ”§ Carregando pipeline otimizado para qualidade...")
        start_time = time.time()
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(600)  # 10 minutos para carregar
        
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )
            
            if torch.cuda.is_available():
                logger.info("ğŸš€ GPU detectada - usando aceleraÃ§Ã£o")
                pipeline = pipeline.to(torch.device("cuda"))
            else:
                logger.info(f"ğŸ–¥ï¸ CPU otimizada ({resource_manager.max_cpu_cores} cores)")
                pipeline = pipeline.to(torch.device("cpu"))
            
            signal.alarm(0)
            
        except TimeoutException:
            logger.error("âŒ Timeout no carregamento do pipeline")
            return intelligent_fallback_diarization(audio_path, duration)
        
        load_time = time.time() - start_time
        logger.info(f"âœ… Pipeline carregado em {load_time:.1f}s")
        
        # Processamento direto para Ã¡udios curtos (â‰¤20 min)
        if duration <= 1200:  # 20 minutos
            logger.info("âš¡ Processamento direto para Ã¡udio curto")
            
            try:
                timeout_min = resource_manager.get_chunk_timeout(duration)
                segments = diarize_chunk_optimized(pipeline, audio_path, "Ã¡udio completo", timeout_min)
                speakers = set(seg.speaker for seg in segments)
                logger.info(f"âœ… PROCESSAMENTO DIRETO: {len(segments)} segmentos, {len(speakers)} speakers")
                return segments
                
            except Exception as e:
                logger.error(f"Falha no processamento direto: {e}")
                return intelligent_fallback_diarization(audio_path, duration)
        
        else:  # Processamento em 3 chunks
            logger.info("ğŸ”ª DivisÃ£o em 3 chunks para Ã¡udio longo")
            
            chunks = split_audio_into_3_chunks(audio_path)
            chunk_results = []
            temp_files = []
            
            try:
                for i, (chunk_path, start_time_chunk, end_time_chunk) in enumerate(chunks):
                    temp_files.append(chunk_path)
                    
                    # Verificar recursos antes de cada chunk
                    resources = resource_manager.check_resources()
                    if resources['memory_percent'] > 85:
                        logger.warning("ğŸ§¹ RAM alta - limpando memÃ³ria")
                        gc.collect()
                        time.sleep(2)
                    
                    chunk_duration = end_time_chunk - start_time_chunk
                    timeout_min = resource_manager.get_chunk_timeout(chunk_duration)
                    chunk_info = f"Chunk {i+1}/3"
                    
                    logger.info(f"ğŸš€ Iniciando {chunk_info} (RAM: {resources['memory_percent']:.1f}%)")
                    
                    # Processar chunk
                    segments = diarize_chunk_optimized(pipeline, chunk_path, chunk_info, timeout_min)
                    chunk_results.append((segments, start_time_chunk, end_time_chunk))
                
                # Mesclar 3 chunks
                final_segments = merge_3_chunks_advanced(chunk_results)
                
                speakers = set(seg.speaker for seg in final_segments)
                logger.info(f"ğŸ¯ PROCESSAMENTO 3-CHUNKS CONCLUÃDO: {len(final_segments)} segmentos, {len(speakers)} speakers")
                return final_segments
                
            finally:
                # Limpeza de arquivos temporÃ¡rios
                for temp_file in temp_files:
                    try:
                        if os.path.exists(temp_file):
                            os.unlink(temp_file)
                    except:
                        pass
                
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
    except Exception as e:
        logger.error(f"âŒ Erro crÃ­tico: {e}")
        return intelligent_fallback_diarization(audio_path, duration)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso: python diarization.py <audio_path>")
        sys.exit(1)
    
    audio_path = sys.argv[1]
    
    try:
        logger.info(f"ğŸ¯ Iniciando diarizaÃ§Ã£o FIXA-3-CHUNKS: {audio_path}")
        segments = diarize_audio(audio_path)
        
        # Output
        for seg in segments:
            print(seg.to_dict())
            
    except Exception as e:
        logger.error(f"âŒ Erro: {e}")
        sys.exit(1)